{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3e1f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:33 --:--:--     0\n",
      "  0 80.2M    0     0    0     0      0      0 --:--:--  0:00:34 --:--:--     0\n",
      "  0 80.2M    0 98304    0     0   2788      0  8:22:54  0:00:35  8:22:19  2788\n",
      "  0 80.2M    0  560k    0     0  15824      0  1:28:36  0:00:36  1:28:00 15825\n",
      "  2 80.2M    2 2368k    0     0  65124      0  0:21:31  0:00:37  0:20:54 65831\n",
      "  5 80.2M    5 4320k    0     0   112k      0  0:12:07  0:00:38  0:11:29  957k\n",
      "  9 80.2M    9 7744k    0     0   197k      0  0:06:56  0:00:39  0:06:17 1561k\n",
      " 13 80.2M   13 10.4M    0     0   265k      0  0:05:09  0:00:40  0:04:29 2126k\n",
      " 16 80.2M   16 13.5M    0     0   335k      0  0:04:05  0:00:41  0:03:24 2652k\n",
      " 21 80.2M   21 16.8M    0     0   406k      0  0:03:21  0:00:42  0:02:39 2833k\n",
      " 24 80.2M   24 19.3M    0     0   458k      0  0:02:59  0:00:43  0:02:16 3097k\n",
      " 26 80.2M   26 21.5M    0     0   498k      0  0:02:44  0:00:44  0:02:00 2857k\n",
      " 29 80.2M   29 23.5M    0     0   532k      0  0:02:34  0:00:45  0:01:49 2677k\n",
      " 32 80.2M   32 25.8M    0     0   573k      0  0:02:23  0:00:46  0:01:37 2535k\n",
      " 35 80.2M   35 28.1M    0     0   609k      0  0:02:14  0:00:47  0:01:27 2426k\n",
      " 37 80.2M   37 30.4M    0     0   647k      0  0:02:06  0:00:48  0:01:18 2280k\n",
      " 40 80.2M   40 32.7M    0     0   681k      0  0:02:00  0:00:49  0:01:11 2300k\n",
      " 43 80.2M   43 34.8M    0     0   711k      0  0:01:55  0:00:50  0:01:05 2330k\n",
      " 46 80.2M   46 37.1M    0     0   742k      0  0:01:50  0:00:51  0:00:59 2310k\n",
      " 49 80.2M   49 39.4M    0     0   772k      0  0:01:46  0:00:52  0:00:54 2316k\n",
      " 52 80.2M   52 41.8M    0     0   805k      0  0:01:41  0:00:53  0:00:48 2331k\n",
      " 55 80.2M   55 44.4M    0     0   838k      0  0:01:37  0:00:54  0:00:43 2389k\n",
      " 58 80.2M   58 47.0M    0     0   873k      0  0:01:34  0:00:55  0:00:39 2497k\n",
      " 62 80.2M   62 50.0M    0     0   911k      0  0:01:30  0:00:56  0:00:34 2640k\n",
      " 66 80.2M   66 53.2M    0     0   951k      0  0:01:26  0:00:57  0:00:29 2824k\n",
      " 69 80.2M   69 55.9M    0     0   983k      0  0:01:23  0:00:58  0:00:25 2883k\n",
      " 73 80.2M   73 58.7M    0     0  1016k      0  0:01:20  0:00:59  0:00:21 2944k\n",
      " 77 80.2M   77 61.9M    0     0  1053k      0  0:01:17  0:01:00  0:00:17 3046k\n",
      " 81 80.2M   81 65.2M    0     0  1090k      0  0:01:15  0:01:01  0:00:14 3107k\n",
      " 85 80.2M   85 68.6M    0     0  1129k      0  0:01:12  0:01:02  0:00:10 3155k\n",
      " 89 80.2M   89 72.0M    0     0  1167k      0  0:01:10  0:01:03  0:00:07 3307k\n",
      " 94 80.2M   94 75.6M    0     0  1206k      0  0:01:08  0:01:04  0:00:04 3456k\n",
      " 98 80.2M   98 79.0M    0     0  1241k      0  0:01:06  0:01:05  0:00:01 3504k\n",
      "100 80.2M  100 80.2M    0     0  1253k      0  0:01:05  0:01:05 --:--:-- 3561k\n",
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz\n",
    "!rm -r aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae99b194",
   "metadata": {},
   "source": [
    "**Preparing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c020d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b3eb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259aaa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    \n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname,\n",
    "                    val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2037cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 61520 files belonging to 3 classes.\n",
      "Found 2880 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", batch_size=batch_size\n",
    ")\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val\", batch_size=batch_size\n",
    ")\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f776410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd8f9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dataset(dataset, percent, total_size_estimate=20000):\n",
    "    \"\"\"نمونه‌گیری تصادفی از tf.data.Dataset بدون تبدیل به لیست\"\"\"\n",
    "    sample_size = max(1, int(percent * total_size_estimate))\n",
    "    indices = sorted(random.sample(range(total_size_estimate), sample_size))\n",
    "\n",
    "    sampled = dataset.enumerate().filter(lambda i, data: tf.reduce_any(i == indices)).map(lambda i, data: data)\n",
    "    return sampled.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711bacc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function sample_dataset.<locals>.<lambda> at 0x000001CF60E7DEA0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function sample_dataset.<locals>.<lambda> at 0x000001CF60E7DEA0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda i, data: data\n",
      "\n",
      "Match 1:\n",
      "lambda i, data: tf.reduce_any(i == indices)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function sample_dataset.<locals>.<lambda> at 0x000001CF60E7DEA0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function sample_dataset.<locals>.<lambda> at 0x000001CF60E7DEA0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda i, data: data\n",
      "\n",
      "Match 1:\n",
      "lambda i, data: tf.reduce_any(i == indices)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function sample_dataset.<locals>.<lambda> at 0x000001CF60E7DAB0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function sample_dataset.<locals>.<lambda> at 0x000001CF60E7DAB0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda i, data: data\n",
      "\n",
      "Match 1:\n",
      "lambda i, data: tf.reduce_any(i == indices)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function sample_dataset.<locals>.<lambda> at 0x000001CF60E7DAB0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function sample_dataset.<locals>.<lambda> at 0x000001CF60E7DAB0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda i, data: data\n",
      "\n",
      "Match 1:\n",
      "lambda i, data: tf.reduce_any(i == indices)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function sample_dataset.<locals>.<lambda> at 0x000001CF34965D80> and will run it as-is.\n",
      "Cause: could not parse the source code of <function sample_dataset.<locals>.<lambda> at 0x000001CF34965D80>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda i, data: data\n",
      "\n",
      "Match 1:\n",
      "lambda i, data: tf.reduce_any(i == indices)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function sample_dataset.<locals>.<lambda> at 0x000001CF34965D80> and will run it as-is.\n",
      "Cause: could not parse the source code of <function sample_dataset.<locals>.<lambda> at 0x000001CF34965D80>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda i, data: data\n",
      "\n",
      "Match 1:\n",
      "lambda i, data: tf.reduce_any(i == indices)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function sample_dataset.<locals>.<lambda> at 0x000001CF34964EE0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function sample_dataset.<locals>.<lambda> at 0x000001CF34964EE0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda i, data: data\n",
      "\n",
      "Match 1:\n",
      "lambda i, data: tf.reduce_any(i == indices)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function sample_dataset.<locals>.<lambda> at 0x000001CF34964EE0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function sample_dataset.<locals>.<lambda> at 0x000001CF34964EE0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda i, data: data\n",
      "\n",
      "Match 1:\n",
      "lambda i, data: tf.reduce_any(i == indices)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function sample_dataset.<locals>.<lambda> at 0x000001CF34967D00> and will run it as-is.\n",
      "Cause: could not parse the source code of <function sample_dataset.<locals>.<lambda> at 0x000001CF34967D00>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda i, data: data\n",
      "\n",
      "Match 1:\n",
      "lambda i, data: tf.reduce_any(i == indices)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function sample_dataset.<locals>.<lambda> at 0x000001CF34967D00> and will run it as-is.\n",
      "Cause: could not parse the source code of <function sample_dataset.<locals>.<lambda> at 0x000001CF34967D00>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda i, data: data\n",
      "\n",
      "Match 1:\n",
      "lambda i, data: tf.reduce_any(i == indices)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function sample_dataset.<locals>.<lambda> at 0x000001CF34966D40> and will run it as-is.\n",
      "Cause: could not parse the source code of <function sample_dataset.<locals>.<lambda> at 0x000001CF34966D40>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda i, data: data\n",
      "\n",
      "Match 1:\n",
      "lambda i, data: tf.reduce_any(i == indices)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function sample_dataset.<locals>.<lambda> at 0x000001CF34966D40> and will run it as-is.\n",
      "Cause: could not parse the source code of <function sample_dataset.<locals>.<lambda> at 0x000001CF34966D40>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda i, data: data\n",
      "\n",
      "Match 1:\n",
      "lambda i, data: tf.reduce_any(i == indices)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "small_train_ds = sample_dataset(train_ds.unbatch(), percent=0.01, total_size_estimate=20000)\n",
    "small_val_ds = sample_dataset(val_ds.unbatch(), percent=0.01, total_size_estimate=5000)\n",
    "small_test_ds = sample_dataset(test_ds.unbatch(), percent=0.01, total_size_estimate=10000)\n",
    "text_only_small_train_ds = small_train_ds.map(lambda x, y: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e193bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (full): 61520 samples\n",
      "Train (small): 200 samples\n",
      "Validation (full): 2880 samples\n",
      "Validation (small): 29 samples\n",
      "Test (full): 25000 samples\n",
      "Test (small): 100 samples\n"
     ]
    }
   ],
   "source": [
    "def count_samples(dataset):\n",
    "    return sum(1 for _ in dataset.unbatch())\n",
    "\n",
    "# شمارش تعداد نمونه‌های دیتاست اصلی\n",
    "num_train_full = count_samples(train_ds)\n",
    "num_val_full = count_samples(val_ds)\n",
    "num_test_full = count_samples(test_ds)\n",
    "\n",
    "# شمارش تعداد نمونه‌های دیتاست کوچک‌شده\n",
    "num_small_train = count_samples(small_train_ds)\n",
    "num_small_val = count_samples(small_val_ds)\n",
    "num_small_test = count_samples(small_test_ds)\n",
    "\n",
    "# نمایش تعداد نمونه‌ها\n",
    "print(f\"Train (full): {num_train_full} samples\")\n",
    "print(f\"Train (small): {num_small_train} samples\")\n",
    "print(f\"Validation (full): {num_val_full} samples\")\n",
    "print(f\"Validation (small): {num_small_val} samples\")\n",
    "print(f\"Test (full): {num_test_full} samples\")\n",
    "print(f\"Test (small): {num_small_test} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8043ad54",
   "metadata": {},
   "source": [
    "**Vectorizing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed7005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "text_vectorization.adapt(text_only_small_train_ds)\n",
    "\n",
    "int_train_ds = small_train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_val_ds = small_val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_test_ds = small_test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb38b2b",
   "metadata": {},
   "source": [
    "**Transformer encoder implemented as a subclassed `Layer`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328b747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2770e645",
   "metadata": {},
   "source": [
    "**Using the Transformer encoder for text classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df9fcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\pouya\\anaconda3\\envs\\octopus310\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">543,776</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │     \u001b[38;5;34m5,120,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m543,776\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,664,033</span> (21.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,664,033\u001b[0m (21.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,664,033</span> (21.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,664,033\u001b[0m (21.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab_size = 20000\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b166df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.1350 - loss: -118.9108 - val_accuracy: 0.6207 - val_loss: 68.1542\n",
      "Epoch 2/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.1167 - loss: -124.9737 - val_accuracy: 0.4483 - val_loss: 101.3130\n",
      "Epoch 3/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.0920 - loss: -143.7188 - val_accuracy: 0.4828 - val_loss: 105.1661\n",
      "Epoch 4/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.0748 - loss: -163.8885 - val_accuracy: 0.6207 - val_loss: 82.1243\n",
      "Epoch 5/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.1353 - loss: -155.1343 - val_accuracy: 0.6207 - val_loss: 87.7971\n",
      "Epoch 6/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.0963 - loss: -171.4715 - val_accuracy: 0.4828 - val_loss: 125.0603\n",
      "Epoch 7/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.1601 - loss: -187.0826 - val_accuracy: 0.5517 - val_loss: 107.7030\n",
      "Epoch 8/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.0963 - loss: -181.2293 - val_accuracy: 0.4828 - val_loss: 146.6662\n",
      "Epoch 9/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.1205 - loss: -210.1224 - val_accuracy: 0.6207 - val_loss: 102.2073\n",
      "Epoch 10/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.0816 - loss: -230.3557 - val_accuracy: 0.5517 - val_loss: 142.5186\n",
      "Epoch 11/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.1383 - loss: -230.3381 - val_accuracy: 0.3793 - val_loss: 200.6472\n",
      "Epoch 12/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step - accuracy: 0.0939 - loss: -237.7693 - val_accuracy: 0.4483 - val_loss: 191.8023\n",
      "Epoch 13/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.0943 - loss: -254.7655 - val_accuracy: 0.4483 - val_loss: 192.7744\n",
      "Epoch 14/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.1422 - loss: -219.3672 - val_accuracy: 0.7586 - val_loss: 89.8445\n",
      "Epoch 15/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.1371 - loss: -242.7780 - val_accuracy: 0.6897 - val_loss: 107.0610\n",
      "Epoch 16/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.1259 - loss: -271.5062 - val_accuracy: 0.6897 - val_loss: 125.8150\n",
      "Epoch 17/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.1598 - loss: -267.4525 - val_accuracy: 0.5862 - val_loss: 178.7971\n",
      "Epoch 18/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.1218 - loss: -277.3148 - val_accuracy: 0.4828 - val_loss: 227.3473\n",
      "Epoch 19/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.0955 - loss: -327.4717 - val_accuracy: 0.5517 - val_loss: 187.7727\n",
      "Epoch 20/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.1432 - loss: -317.5388 - val_accuracy: 0.4138 - val_loss: 272.6088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1cf34a26890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"transformer_encoder.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dca6efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 901ms/step - accuracy: 0.4981 - loss: 85.5045\n",
      "Test acc: 0.490\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(\n",
    "    \"transformer_encoder.keras\",\n",
    "    custom_objects={\"TransformerEncoder\": TransformerEncoder})\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2b58c7",
   "metadata": {},
   "source": [
    "#### Using positional encoding to re-inject order information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1edb19",
   "metadata": {},
   "source": [
    "**Implementing positional embedding as a subclassed layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0124b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # فقط همینجا mask_zero=True می‌دهیم، نیازی به compute_mask نداریم\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim,\n",
    "            output_dim=output_dim,\n",
    "            mask_zero=True\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length,\n",
    "            output_dim=output_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens    = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    # متد compute_mask را حذف کردم\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
