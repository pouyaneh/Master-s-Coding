{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3e1f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:33 --:--:--     0\n",
      "  0 80.2M    0     0    0     0      0      0 --:--:--  0:00:34 --:--:--     0\n",
      "  0 80.2M    0 98304    0     0   2788      0  8:22:54  0:00:35  8:22:19  2788\n",
      "  0 80.2M    0  560k    0     0  15824      0  1:28:36  0:00:36  1:28:00 15825\n",
      "  2 80.2M    2 2368k    0     0  65124      0  0:21:31  0:00:37  0:20:54 65831\n",
      "  5 80.2M    5 4320k    0     0   112k      0  0:12:07  0:00:38  0:11:29  957k\n",
      "  9 80.2M    9 7744k    0     0   197k      0  0:06:56  0:00:39  0:06:17 1561k\n",
      " 13 80.2M   13 10.4M    0     0   265k      0  0:05:09  0:00:40  0:04:29 2126k\n",
      " 16 80.2M   16 13.5M    0     0   335k      0  0:04:05  0:00:41  0:03:24 2652k\n",
      " 21 80.2M   21 16.8M    0     0   406k      0  0:03:21  0:00:42  0:02:39 2833k\n",
      " 24 80.2M   24 19.3M    0     0   458k      0  0:02:59  0:00:43  0:02:16 3097k\n",
      " 26 80.2M   26 21.5M    0     0   498k      0  0:02:44  0:00:44  0:02:00 2857k\n",
      " 29 80.2M   29 23.5M    0     0   532k      0  0:02:34  0:00:45  0:01:49 2677k\n",
      " 32 80.2M   32 25.8M    0     0   573k      0  0:02:23  0:00:46  0:01:37 2535k\n",
      " 35 80.2M   35 28.1M    0     0   609k      0  0:02:14  0:00:47  0:01:27 2426k\n",
      " 37 80.2M   37 30.4M    0     0   647k      0  0:02:06  0:00:48  0:01:18 2280k\n",
      " 40 80.2M   40 32.7M    0     0   681k      0  0:02:00  0:00:49  0:01:11 2300k\n",
      " 43 80.2M   43 34.8M    0     0   711k      0  0:01:55  0:00:50  0:01:05 2330k\n",
      " 46 80.2M   46 37.1M    0     0   742k      0  0:01:50  0:00:51  0:00:59 2310k\n",
      " 49 80.2M   49 39.4M    0     0   772k      0  0:01:46  0:00:52  0:00:54 2316k\n",
      " 52 80.2M   52 41.8M    0     0   805k      0  0:01:41  0:00:53  0:00:48 2331k\n",
      " 55 80.2M   55 44.4M    0     0   838k      0  0:01:37  0:00:54  0:00:43 2389k\n",
      " 58 80.2M   58 47.0M    0     0   873k      0  0:01:34  0:00:55  0:00:39 2497k\n",
      " 62 80.2M   62 50.0M    0     0   911k      0  0:01:30  0:00:56  0:00:34 2640k\n",
      " 66 80.2M   66 53.2M    0     0   951k      0  0:01:26  0:00:57  0:00:29 2824k\n",
      " 69 80.2M   69 55.9M    0     0   983k      0  0:01:23  0:00:58  0:00:25 2883k\n",
      " 73 80.2M   73 58.7M    0     0  1016k      0  0:01:20  0:00:59  0:00:21 2944k\n",
      " 77 80.2M   77 61.9M    0     0  1053k      0  0:01:17  0:01:00  0:00:17 3046k\n",
      " 81 80.2M   81 65.2M    0     0  1090k      0  0:01:15  0:01:01  0:00:14 3107k\n",
      " 85 80.2M   85 68.6M    0     0  1129k      0  0:01:12  0:01:02  0:00:10 3155k\n",
      " 89 80.2M   89 72.0M    0     0  1167k      0  0:01:10  0:01:03  0:00:07 3307k\n",
      " 94 80.2M   94 75.6M    0     0  1206k      0  0:01:08  0:01:04  0:00:04 3456k\n",
      " 98 80.2M   98 79.0M    0     0  1241k      0  0:01:06  0:01:05  0:00:01 3504k\n",
      "100 80.2M  100 80.2M    0     0  1253k      0  0:01:05  0:01:05 --:--:-- 3561k\n",
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz\n",
    "!rm -r aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae99b194",
   "metadata": {},
   "source": [
    "**Preparing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c020d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b3eb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259aaa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    \n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname,\n",
    "                    val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2037cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 61520 files belonging to 3 classes.\n",
      "Found 2880 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", batch_size=batch_size\n",
    ")\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val\", batch_size=batch_size\n",
    ")\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f776410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd8f9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dataset(dataset, percent, total_size_estimate=20000):\n",
    "    \"\"\"نمونه‌گیری تصادفی از tf.data.Dataset بدون تبدیل به لیست\"\"\"\n",
    "    sample_size = max(1, int(percent * total_size_estimate))\n",
    "    indices = sorted(random.sample(range(total_size_estimate), sample_size))\n",
    "\n",
    "    sampled = dataset.enumerate().filter(lambda i, data: tf.reduce_any(i == indices)).map(lambda i, data: data)\n",
    "    return sampled.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711bacc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function sample_dataset.<locals>.<lambda> at 0x000001CF60E7DEA0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function sample_dataset.<locals>.<lambda> at 0x000001CF60E7DEA0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda i, data: data\n",
      "\n",
      "Match 1:\n",
      "lambda i, data: tf.reduce_any(i == indices)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function sample_dataset.<locals>.<lambda> at 0x000001CF60E7DEA0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function sample_dataset.<locals>.<lambda> at 0x000001CF60E7DEA0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda i, data: data\n",
      "\n",
      "Match 1:\n",
      "lambda i, data: tf.reduce_any(i == indices)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function sample_dataset.<locals>.<lambda> at 0x000001CF60E7DAB0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function sample_dataset.<locals>.<lambda> at 0x000001CF60E7DAB0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda i, data: data\n",
      "\n",
      "Match 1:\n",
      "lambda i, data: tf.reduce_any(i == indices)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function sample_dataset.<locals>.<lambda> at 0x000001CF60E7DAB0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function sample_dataset.<locals>.<lambda> at 0x000001CF60E7DAB0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda i, data: data\n",
      "\n",
      "Match 1:\n",
      "lambda i, data: tf.reduce_any(i == indices)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function sample_dataset.<locals>.<lambda> at 0x000001CF34965D80> and will run it as-is.\n",
      "Cause: could not parse the source code of <function sample_dataset.<locals>.<lambda> at 0x000001CF34965D80>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda i, data: data\n",
      "\n",
      "Match 1:\n",
      "lambda i, data: tf.reduce_any(i == indices)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function sample_dataset.<locals>.<lambda> at 0x000001CF34965D80> and will run it as-is.\n",
      "Cause: could not parse the source code of <function sample_dataset.<locals>.<lambda> at 0x000001CF34965D80>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda i, data: data\n",
      "\n",
      "Match 1:\n",
      "lambda i, data: tf.reduce_any(i == indices)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function sample_dataset.<locals>.<lambda> at 0x000001CF34964EE0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function sample_dataset.<locals>.<lambda> at 0x000001CF34964EE0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda i, data: data\n",
      "\n",
      "Match 1:\n",
      "lambda i, data: tf.reduce_any(i == indices)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function sample_dataset.<locals>.<lambda> at 0x000001CF34964EE0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function sample_dataset.<locals>.<lambda> at 0x000001CF34964EE0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda i, data: data\n",
      "\n",
      "Match 1:\n",
      "lambda i, data: tf.reduce_any(i == indices)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function sample_dataset.<locals>.<lambda> at 0x000001CF34967D00> and will run it as-is.\n",
      "Cause: could not parse the source code of <function sample_dataset.<locals>.<lambda> at 0x000001CF34967D00>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda i, data: data\n",
      "\n",
      "Match 1:\n",
      "lambda i, data: tf.reduce_any(i == indices)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function sample_dataset.<locals>.<lambda> at 0x000001CF34967D00> and will run it as-is.\n",
      "Cause: could not parse the source code of <function sample_dataset.<locals>.<lambda> at 0x000001CF34967D00>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda i, data: data\n",
      "\n",
      "Match 1:\n",
      "lambda i, data: tf.reduce_any(i == indices)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function sample_dataset.<locals>.<lambda> at 0x000001CF34966D40> and will run it as-is.\n",
      "Cause: could not parse the source code of <function sample_dataset.<locals>.<lambda> at 0x000001CF34966D40>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda i, data: data\n",
      "\n",
      "Match 1:\n",
      "lambda i, data: tf.reduce_any(i == indices)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function sample_dataset.<locals>.<lambda> at 0x000001CF34966D40> and will run it as-is.\n",
      "Cause: could not parse the source code of <function sample_dataset.<locals>.<lambda> at 0x000001CF34966D40>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda i, data: data\n",
      "\n",
      "Match 1:\n",
      "lambda i, data: tf.reduce_any(i == indices)\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "small_train_ds = sample_dataset(train_ds.unbatch(), percent=0.01, total_size_estimate=20000)\n",
    "small_val_ds = sample_dataset(val_ds.unbatch(), percent=0.01, total_size_estimate=5000)\n",
    "small_test_ds = sample_dataset(test_ds.unbatch(), percent=0.01, total_size_estimate=10000)\n",
    "text_only_small_train_ds = small_train_ds.map(lambda x, y: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e193bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (full): 61520 samples\n",
      "Train (small): 200 samples\n",
      "Validation (full): 2880 samples\n",
      "Validation (small): 29 samples\n",
      "Test (full): 25000 samples\n",
      "Test (small): 100 samples\n"
     ]
    }
   ],
   "source": [
    "def count_samples(dataset):\n",
    "    return sum(1 for _ in dataset.unbatch())\n",
    "\n",
    "# شمارش تعداد نمونه‌های دیتاست اصلی\n",
    "num_train_full = count_samples(train_ds)\n",
    "num_val_full = count_samples(val_ds)\n",
    "num_test_full = count_samples(test_ds)\n",
    "\n",
    "# شمارش تعداد نمونه‌های دیتاست کوچک‌شده\n",
    "num_small_train = count_samples(small_train_ds)\n",
    "num_small_val = count_samples(small_val_ds)\n",
    "num_small_test = count_samples(small_test_ds)\n",
    "\n",
    "# نمایش تعداد نمونه‌ها\n",
    "print(f\"Train (full): {num_train_full} samples\")\n",
    "print(f\"Train (small): {num_small_train} samples\")\n",
    "print(f\"Validation (full): {num_val_full} samples\")\n",
    "print(f\"Validation (small): {num_small_val} samples\")\n",
    "print(f\"Test (full): {num_test_full} samples\")\n",
    "print(f\"Test (small): {num_small_test} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8043ad54",
   "metadata": {},
   "source": [
    "**Vectorizing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed7005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "text_vectorization.adapt(text_only_small_train_ds)\n",
    "\n",
    "int_train_ds = small_train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_val_ds = small_val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_test_ds = small_test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
