{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63d0e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, precision_score, roc_curve, precision_recall_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c974422",
   "metadata": {},
   "source": [
    "Graph Split Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d186bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_graph(G, train_frac=0.8, test_frac=0.1, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    edges = list(G.edges())\n",
    "    np.random.shuffle(edges)\n",
    "    m = len(edges)\n",
    "    n_train = int(train_frac * m)\n",
    "    n_test = int(test_frac * m)\n",
    "\n",
    "    train_edges = edges[:n_train]\n",
    "    test_pos = edges[n_train:n_train + n_test]\n",
    "\n",
    "    G_train = nx.Graph()\n",
    "    G_train.add_nodes_from(G.nodes())\n",
    "    G_train.add_edges_from(train_edges)\n",
    "\n",
    "    non_edges = list(nx.non_edges(G_train))\n",
    "    np.random.shuffle(non_edges)\n",
    "    test_neg = non_edges[:n_test]\n",
    "\n",
    "    return G_train, test_pos, test_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48edb415",
   "metadata": {},
   "source": [
    "Scoring methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf381f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_method(G_train, u, v, method):\n",
    "    if method == 'common_neighbors':\n",
    "        return len(list(nx.common_neighbors(G_train, u, v)))\n",
    "    if method == 'jaccard':\n",
    "        return next(nx.jaccard_coefficient(G_train, [(u, v)]))[2]\n",
    "    if method == 'adamic_adar':\n",
    "        return next(nx.adamic_adar_index(G_train, [(u, v)]))[2]\n",
    "    if method == 'pref_attachment':\n",
    "        return next(nx.preferential_attachment(G_train, [(u, v)]))[2]\n",
    "    if method == 'resource_allocation':\n",
    "        return next(nx.resource_allocation_index(G_train, [(u, v)]))[2]\n",
    "    raise ValueError(f\"Unknown method {method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421a6dba",
   "metadata": {},
   "source": [
    "Method evaluation with detailed scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c4c64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_method(G_train, test_pos, test_neg, method):\n",
    "    pairs = test_pos + test_neg\n",
    "    y_true = np.array([1]*len(test_pos) + [0]*len(test_neg))\n",
    "    scores = np.array([score_method(G_train, u, v, method) for u, v in pairs])\n",
    "\n",
    "    # ROC AUC\n",
    "    auc_score = roc_auc_score(y_true, scores)\n",
    "    # ROC curve data\n",
    "    fpr, tpr, _ = roc_curve(y_true, scores)\n",
    "\n",
    "    # Precision-Recall AUC\n",
    "    precision_vals, recall_vals, _ = precision_recall_curve(y_true, scores)\n",
    "    pr_auc = auc(recall_vals, precision_vals)\n",
    "\n",
    "    # Precision@k\n",
    "    k = len(test_pos)\n",
    "    idx_top = np.argsort(scores)[::-1][:k]\n",
    "    y_pred_top = np.zeros_like(y_true)\n",
    "    y_pred_top[idx_top] = 1\n",
    "    precision_k = precision_score(y_true, y_pred_top)\n",
    "\n",
    "    return {\n",
    "        'method': method,\n",
    "        'AUC': auc_score,\n",
    "        'PR_AUC': pr_auc,\n",
    "        'Precision@k': precision_k,\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr,\n",
    "        'precision_curve': precision_vals,\n",
    "        'recall_curve': recall_vals\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166ce213",
   "metadata": {},
   "source": [
    "Comparison across methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6abc61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_link_prediction(G, methods=None, train_frac=0.8, test_frac=0.1, seed=42):\n",
    "    methods = methods or [\n",
    "        'common_neighbors',\n",
    "        'jaccard',\n",
    "        'adamic_adar',\n",
    "        'pref_attachment',\n",
    "        'resource_allocation'\n",
    "    ]\n",
    "    G_train, test_pos, test_neg = train_test_split_graph(G, train_frac, test_frac, seed)\n",
    "    detail = [evaluate_method(G_train, test_pos, test_neg, m) for m in methods]\n",
    "    summary = pd.DataFrame([\n",
    "        {\n",
    "            'method': r['method'],\n",
    "            'AUC': r['AUC'],\n",
    "            'PR_AUC': r['PR_AUC'],\n",
    "            'Precision@k': r['Precision@k']\n",
    "        }\n",
    "        for r in detail\n",
    "    ])\n",
    "    return summary, detail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca023e4a",
   "metadata": {},
   "source": [
    "Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c81923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curves(detail_results):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for res in detail_results:\n",
    "        plt.plot(res['fpr'], res['tpr'], label=f\"{res['method']} (AUC={res['AUC']:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.title('ROC Curves')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8675dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_bars(df):\n",
    "    df_plot = df.set_index('method')\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    df_plot['AUC'].plot(kind='bar', ax=axes[0], title='ROC AUC Comparison')\n",
    "    axes[0].set_ylabel('AUC')\n",
    "    df_plot['PR_AUC'].plot(kind='bar', ax=axes[1], title='PR AUC Comparison')\n",
    "    axes[1].set_ylabel('PR AUC')\n",
    "    df_plot['Precision@k'].plot(kind='bar', ax=axes[2], title='Precision@k Comparison')\n",
    "    axes[2].set_ylabel('Precision@k')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
