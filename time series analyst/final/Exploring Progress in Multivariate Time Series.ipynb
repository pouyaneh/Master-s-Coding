{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9c11289",
   "metadata": {},
   "source": [
    "Basic Imports and Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb84fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77ad5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54ce735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn for Preprocessing and Metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5580116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f592f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Settings\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5069427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility: Set random seed for consistent results\n",
    "SEED = 42\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d024f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device Setup: Enable GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbc51f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Display CUDA device information\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Device Count: {torch.cuda.device_count()}\")\n",
    "    print(f\"CUDA Available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d499f52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to check memory usage (if needed during training)\n",
    "def print_memory_usage():\n",
    "    if device.type == \"cuda\":\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**2\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**2\n",
    "        print(f\"Memory Allocated: {allocated:.2f} MB\")\n",
    "        print(f\"Memory Reserved: {reserved:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d1ad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming imports and environment setup\n",
    "print(\"Basic imports and device setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28158f9c",
   "metadata": {},
   "source": [
    "Dataset Class, Loading, and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3977ba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset Class\n",
    "class MTSDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for Multivariate Time Series (MTS) forecasting.\n",
    "\n",
    "    Args:\n",
    "        data (numpy.ndarray): Raw time series data (shape: [samples, features]).\n",
    "        seq_length (int): Length of the input sequence for forecasting.\n",
    "        forecast_length (int): Length of the prediction horizon.\n",
    "        normalize (bool): Whether to apply normalization to the data.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, seq_length, forecast_length, normalize=True):\n",
    "        if data is None or data.size == 0:\n",
    "            raise ValueError(\"Data is empty or not properly loaded.\")\n",
    "        \n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "        self.forecast_length = forecast_length\n",
    "        self.normalize = normalize\n",
    "\n",
    "        # Normalize data using StandardScaler if enabled\n",
    "        if self.normalize:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.data = self.scaler.fit_transform(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Calculate dataset length considering input and forecast horizons\n",
    "        return len(self.data) - self.seq_length - self.forecast_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract input (X) and target (Y) sequences\n",
    "        x = self.data[idx: idx + self.seq_length]\n",
    "        y = self.data[idx + self.seq_length: idx + self.seq_length + self.forecast_length]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6300e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic Dataset Download Function\n",
    "def download_datasets(dataset_names, save_path=\"datasets\"):\n",
    "    \"\"\"\n",
    "    Automatically download multiple datasets for MTS forecasting.\n",
    "\n",
    "    Args:\n",
    "        dataset_names (list): List of dataset names to download (e.g., ['ETT', 'METR-LA']).\n",
    "        save_path (str): Directory to save the downloaded datasets.\n",
    "\n",
    "    Returns:\n",
    "        dict: Paths to the downloaded datasets.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    dataset_urls = {\n",
    "        \"ETT\": \"https://raw.githubusercontent.com/zhouhaoyi/ETDataset/main/ETT-small/ETTm2.csv\",\n",
    "        \"METR-LA\": \"https://raw.githubusercontent.com/liyaguang/DCRNN/master/data/sensor_graph/adj_mx.pkl\",\n",
    "        \"PEMS-BAY\": \"https://raw.githubusercontent.com/liyaguang/DCRNN/master/data/sensor_graph/adj_mx_bay.pkl\"\n",
    "    }\n",
    "\n",
    "    downloaded_paths = {}\n",
    "    for dataset_name in dataset_names:\n",
    "        if dataset_name not in dataset_urls:\n",
    "            print(f\"Dataset {dataset_name} is not available for automatic download.\")\n",
    "            continue\n",
    "\n",
    "        url = dataset_urls[dataset_name]\n",
    "        file_name = os.path.join(save_path, os.path.basename(url))\n",
    "\n",
    "        # Download the dataset if it doesn't already exist\n",
    "        if not os.path.exists(file_name):\n",
    "            print(f\"Downloading {dataset_name} dataset from {url}...\")\n",
    "            try:\n",
    "                response = requests.get(url, stream=True)\n",
    "                response.raise_for_status()  # Raise an HTTPError for bad responses\n",
    "                with open(file_name, \"wb\") as f:\n",
    "                    for chunk in response.iter_content(chunk_size=1024):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "                print(f\"{dataset_name} dataset downloaded successfully.\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Failed to download {dataset_name} dataset. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        downloaded_paths[dataset_name] = file_name\n",
    "\n",
    "    return downloaded_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17c455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset Function with Format-Specific Handling\n",
    "def load_data(dataset_name, file_path, delimiter=','):\n",
    "    \"\"\"\n",
    "    Load and preprocess datasets, adapting to specific formats (e.g., .csv, .pkl).\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset being loaded.\n",
    "        file_path (str): Path to the dataset file.\n",
    "        delimiter (str): Delimiter used in the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray or dictionary: Loaded dataset as a NumPy array (for numerical datasets) \n",
    "                                      or dictionary (for .pkl datasets).\n",
    "    \"\"\"\n",
    "    print(f\"Loading {dataset_name} dataset from {file_path}...\")\n",
    "\n",
    "    try:\n",
    "        # Handle binary `.pkl` files (e.g., METR-LA, PEMS-BAY)\n",
    "        if dataset_name in [\"METR-LA\", \"PEMS-BAY\"]:\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                data = pickle.load(f, encoding=\"latin1\")  # Use 'latin1' to properly decode non-ASCII characters\n",
    "            print(f\"{dataset_name} dataset loaded successfully as a dictionary.\")\n",
    "            return data\n",
    "\n",
    "        # Handle CSV files (e.g., ETT)\n",
    "        elif dataset_name == \"ETT\":\n",
    "            data = pd.read_csv(file_path, sep=delimiter, on_bad_lines=\"skip\")\n",
    "            if isinstance(data.iloc[0, 0], str):\n",
    "                print(\"Detected timestamp column. Excluding it from the dataset.\")\n",
    "                data = data.iloc[:, 1:]  # Exclude timestamp column\n",
    "            if data.empty:\n",
    "                raise ValueError(f\"The dataset at {file_path} is empty or not properly formatted.\")\n",
    "            return data.values\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Loading logic for {dataset_name} is not yet implemented.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: An issue occurred while loading the {dataset_name} dataset.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae80926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage: Download and Load Multiple Datasets\n",
    "try:\n",
    "    dataset_names = [\"ETT\", \"METR-LA\", \"PEMS-BAY\"]\n",
    "\n",
    "    downloaded_paths = download_datasets(dataset_names)\n",
    "\n",
    "    loaded_data = {}\n",
    "    for dataset_name, file_path in downloaded_paths.items():\n",
    "        raw_data = load_data(dataset_name, file_path)\n",
    "\n",
    "        if dataset_name == \"ETT\":\n",
    "            seq_length = 12  # Length of historical input sequence\n",
    "            forecast_length = 12  # Length of prediction horizon\n",
    "            dataset = MTSDataset(raw_data, seq_length=seq_length, forecast_length=forecast_length)\n",
    "            loaded_data[dataset_name] = dataset\n",
    "            print(f\"ETT Dataset initialized with {len(dataset)} samples.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d04258",
   "metadata": {},
   "source": [
    "BasicTS+ Benchmark: Unified Training Pipeline (Incorporating dataloaders, runners, normalization, training tricks, and evaluation standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415942a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTSPlusTrainer:\n",
    "    \"\"\"\n",
    "    Unified Training Pipeline for BasicTS+ Benchmark.\n",
    "    \n",
    "    Incorporates standardized data loading, normalization, curriculum learning, gradient clipping,\n",
    "    and evaluation standardization for fair and reproducible benchmarking.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, dataloader, optimizer, criterion, scheduler=None, \n",
    "                 clip_grad=1.0, use_curriculum=True, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        \"\"\"\n",
    "        Initialize the unified training pipeline.\n",
    "        \n",
    "        Args:\n",
    "            model (torch.nn.Module): The time series forecasting model.\n",
    "            dataloader (torch.utils.data.DataLoader): Dataloader for training data.\n",
    "            optimizer (torch.optim.Optimizer): Optimizer for model training.\n",
    "            criterion (torch.nn.Module): Loss function.\n",
    "            scheduler (torch.optim.lr_scheduler, optional): Learning rate scheduler. Defaults to None.\n",
    "            clip_grad (float): Gradient clipping value. Defaults to 1.0.\n",
    "            use_curriculum (bool): Whether to use curriculum learning. Defaults to True.\n",
    "            device (str): Computation device. Defaults to \"cuda\" if available.\n",
    "        \"\"\"\n",
    "        self.model = model.to(device)\n",
    "        self.dataloader = dataloader\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.scheduler = scheduler\n",
    "        self.clip_grad = clip_grad\n",
    "        self.use_curriculum = use_curriculum\n",
    "        self.device = device\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        \"\"\"\n",
    "        Perform a single training step.\n",
    "        \n",
    "        Args:\n",
    "            batch (tuple): Contains (input sequence, target sequence).\n",
    "        \n",
    "        Returns:\n",
    "            float: Training loss for the batch.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        x, y = batch\n",
    "        x, y = x.to(self.device), y.to(self.device)\n",
    "        \n",
    "        output = self.model(x)\n",
    "        loss = self.criterion(output, y)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Apply gradient clipping\n",
    "        if self.clip_grad:\n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(), self.clip_grad)\n",
    "\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "    def train(self, epochs=50):\n",
    "        \"\"\"\n",
    "        Train the model for a given number of epochs using curriculum learning.\n",
    "        \n",
    "        Args:\n",
    "            epochs (int): Number of training epochs. Defaults to 50.\n",
    "        \"\"\"\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            total_loss = 0.0\n",
    "            for batch_idx, batch in enumerate(self.dataloader):\n",
    "                # Implement curriculum learning (optional)\n",
    "                if self.use_curriculum and epoch < epochs // 2:\n",
    "                    batch = self.modify_batch_difficulty(batch, factor=epoch / epochs)\n",
    "                \n",
    "                batch_loss = self.train_step(batch)\n",
    "                total_loss += batch_loss\n",
    "\n",
    "            avg_loss = total_loss / len(self.dataloader)\n",
    "            \n",
    "            if self.scheduler:\n",
    "                self.scheduler.step()\n",
    "\n",
    "            print(f\"Epoch {epoch}/{epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    def modify_batch_difficulty(self, batch, factor):\n",
    "        \"\"\"\n",
    "        Adjust batch difficulty in curriculum learning.\n",
    "        \n",
    "        Args:\n",
    "            batch (tuple): Contains (input sequence, target sequence).\n",
    "            factor (float): Difficulty scaling factor.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Modified batch.\n",
    "        \"\"\"\n",
    "        x, y = batch\n",
    "        scaled_x = x * factor  # Scale input data based on progression\n",
    "        return scaled_x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af8d30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage:\n",
    "def initialize_training_pipeline(model, train_data, lr=0.001):\n",
    "    \"\"\"\n",
    "    Initialize BasicTS+ training pipeline with model and training dataset.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Time series forecasting model.\n",
    "        train_data (torch.utils.data.Dataset): Training dataset.\n",
    "        lr (float): Learning rate. Defaults to 0.001.\n",
    "    \n",
    "    Returns:\n",
    "        BasicTSPlusTrainer: Initialized training pipeline.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.7)\n",
    "    \n",
    "    trainer = BasicTSPlusTrainer(model, dataloader, optimizer, criterion, scheduler)\n",
    "    return trainer\n",
    "\n",
    "print(\"BasicTS+ Benchmark: Unified Training Pipeline initialized successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "panda38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
