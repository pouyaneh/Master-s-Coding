{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9c11289",
   "metadata": {},
   "source": [
    "Basic Imports and Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb84fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77ad5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54ce735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn for Preprocessing and Metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5580116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f592f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Settings\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5069427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility: Set random seed for consistent results\n",
    "SEED = 42\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d024f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device Setup: Enable GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbc51f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Display CUDA device information\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Device Count: {torch.cuda.device_count()}\")\n",
    "    print(f\"CUDA Available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d499f52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to check memory usage (if needed during training)\n",
    "def print_memory_usage():\n",
    "    if device.type == \"cuda\":\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**2\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**2\n",
    "        print(f\"Memory Allocated: {allocated:.2f} MB\")\n",
    "        print(f\"Memory Reserved: {reserved:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d1ad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming imports and environment setup\n",
    "print(\"Basic imports and device setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28158f9c",
   "metadata": {},
   "source": [
    "Dataset Class, Loading, and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3977ba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset Class\n",
    "class MTSDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for Multivariate Time Series (MTS) forecasting.\n",
    "\n",
    "    Args:\n",
    "        data (numpy.ndarray): Raw time series data (shape: [samples, features]).\n",
    "        seq_length (int): Length of the input sequence for forecasting.\n",
    "        forecast_length (int): Length of the prediction horizon.\n",
    "        normalize (bool): Whether to apply normalization to the data.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, seq_length, forecast_length, normalize=True):\n",
    "        if data is None or data.size == 0:\n",
    "            raise ValueError(\"Data is empty or not properly loaded.\")\n",
    "        \n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "        self.forecast_length = forecast_length\n",
    "        self.normalize = normalize\n",
    "\n",
    "        # Normalize data using StandardScaler if enabled\n",
    "        if self.normalize:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.data = self.scaler.fit_transform(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Calculate dataset length considering input and forecast horizons\n",
    "        return len(self.data) - self.seq_length - self.forecast_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract input (X) and target (Y) sequences\n",
    "        x = self.data[idx: idx + self.seq_length]\n",
    "        y = self.data[idx + self.seq_length: idx + self.seq_length + self.forecast_length]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6300e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic Dataset Download Function\n",
    "def download_datasets(dataset_names, save_path=\"datasets\"):\n",
    "    \"\"\"\n",
    "    Automatically download multiple datasets for MTS forecasting.\n",
    "\n",
    "    Args:\n",
    "        dataset_names (list): List of dataset names to download (e.g., ['ETT', 'METR-LA']).\n",
    "        save_path (str): Directory to save the downloaded datasets.\n",
    "\n",
    "    Returns:\n",
    "        dict: Paths to the downloaded datasets.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    dataset_urls = {\n",
    "        \"ETT\": \"https://raw.githubusercontent.com/zhouhaoyi/ETDataset/main/ETT-small/ETTm2.csv\",\n",
    "        \"METR-LA\": \"https://raw.githubusercontent.com/liyaguang/DCRNN/master/data/sensor_graph/adj_mx.pkl\",\n",
    "        \"PEMS-BAY\": \"https://raw.githubusercontent.com/liyaguang/DCRNN/master/data/sensor_graph/adj_mx_bay.pkl\"\n",
    "    }\n",
    "\n",
    "    downloaded_paths = {}\n",
    "    for dataset_name in dataset_names:\n",
    "        if dataset_name not in dataset_urls:\n",
    "            print(f\"Dataset {dataset_name} is not available for automatic download.\")\n",
    "            continue\n",
    "\n",
    "        url = dataset_urls[dataset_name]\n",
    "        file_name = os.path.join(save_path, os.path.basename(url))\n",
    "\n",
    "        # Download the dataset if it doesn't already exist\n",
    "        if not os.path.exists(file_name):\n",
    "            print(f\"Downloading {dataset_name} dataset from {url}...\")\n",
    "            try:\n",
    "                response = requests.get(url, stream=True)\n",
    "                response.raise_for_status()  # Raise an HTTPError for bad responses\n",
    "                with open(file_name, \"wb\") as f:\n",
    "                    for chunk in response.iter_content(chunk_size=1024):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "                print(f\"{dataset_name} dataset downloaded successfully.\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Failed to download {dataset_name} dataset. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        downloaded_paths[dataset_name] = file_name\n",
    "\n",
    "    return downloaded_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17c455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset Function with Format-Specific Handling\n",
    "def load_data(dataset_name, file_path, delimiter=','):\n",
    "    \"\"\"\n",
    "    Load and preprocess datasets, adapting to specific formats (e.g., .csv, .pkl).\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset being loaded.\n",
    "        file_path (str): Path to the dataset file.\n",
    "        delimiter (str): Delimiter used in the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray or dictionary: Loaded dataset as a NumPy array (for numerical datasets) \n",
    "                                      or dictionary (for .pkl datasets).\n",
    "    \"\"\"\n",
    "    print(f\"Loading {dataset_name} dataset from {file_path}...\")\n",
    "\n",
    "    try:\n",
    "        # Handle binary `.pkl` files (e.g., METR-LA, PEMS-BAY)\n",
    "        if dataset_name in [\"METR-LA\", \"PEMS-BAY\"]:\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                data = pickle.load(f, encoding=\"latin1\")  # Use 'latin1' to properly decode non-ASCII characters\n",
    "            print(f\"{dataset_name} dataset loaded successfully as a dictionary.\")\n",
    "            return data\n",
    "\n",
    "        # Handle CSV files (e.g., ETT)\n",
    "        elif dataset_name == \"ETT\":\n",
    "            data = pd.read_csv(file_path, sep=delimiter, on_bad_lines=\"skip\")\n",
    "            if isinstance(data.iloc[0, 0], str):\n",
    "                print(\"Detected timestamp column. Excluding it from the dataset.\")\n",
    "                data = data.iloc[:, 1:]  # Exclude timestamp column\n",
    "            if data.empty:\n",
    "                raise ValueError(f\"The dataset at {file_path} is empty or not properly formatted.\")\n",
    "            return data.values\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Loading logic for {dataset_name} is not yet implemented.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: An issue occurred while loading the {dataset_name} dataset.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae80926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage: Download and Load Multiple Datasets\n",
    "try:\n",
    "    dataset_names = [\"ETT\", \"METR-LA\", \"PEMS-BAY\"]\n",
    "\n",
    "    downloaded_paths = download_datasets(dataset_names)\n",
    "\n",
    "    loaded_data = {}\n",
    "    for dataset_name, file_path in downloaded_paths.items():\n",
    "        raw_data = load_data(dataset_name, file_path)\n",
    "\n",
    "        if dataset_name == \"ETT\":\n",
    "            seq_length = 12  # Length of historical input sequence\n",
    "            forecast_length = 12  # Length of prediction horizon\n",
    "            dataset = MTSDataset(raw_data, seq_length=seq_length, forecast_length=forecast_length)\n",
    "            loaded_data[dataset_name] = dataset\n",
    "            print(f\"ETT Dataset initialized with {len(dataset)} samples.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "panda38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
