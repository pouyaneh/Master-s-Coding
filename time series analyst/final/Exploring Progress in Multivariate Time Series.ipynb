{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9c11289",
   "metadata": {},
   "source": [
    "Basic Imports and Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb84fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77ad5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54ce735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn for Preprocessing and Metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5580116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f592f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Settings\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5069427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility: Set random seed for consistent results\n",
    "SEED = 42\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d024f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device Setup: Enable GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbc51f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Display CUDA device information\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Device Count: {torch.cuda.device_count()}\")\n",
    "    print(f\"CUDA Available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d499f52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to check memory usage (if needed during training)\n",
    "def print_memory_usage():\n",
    "    if device.type == \"cuda\":\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**2\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**2\n",
    "        print(f\"Memory Allocated: {allocated:.2f} MB\")\n",
    "        print(f\"Memory Reserved: {reserved:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d1ad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming imports and environment setup\n",
    "print(\"Basic imports and device setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28158f9c",
   "metadata": {},
   "source": [
    "Dataset Class, Loading, and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3977ba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset Class\n",
    "class MTSDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for Multivariate Time Series (MTS) forecasting.\n",
    "\n",
    "    Args:\n",
    "        data (numpy.ndarray): Raw time series data (shape: [samples, features]).\n",
    "        seq_length (int): Length of the input sequence for forecasting.\n",
    "        forecast_length (int): Length of the prediction horizon.\n",
    "        normalize (bool): Whether to apply normalization to the data.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, seq_length, forecast_length, normalize=True):\n",
    "        if data is None or data.size == 0:\n",
    "            raise ValueError(\"Data is empty or not properly loaded.\")\n",
    "        \n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "        self.forecast_length = forecast_length\n",
    "        self.normalize = normalize\n",
    "\n",
    "        # Normalize data using StandardScaler if enabled\n",
    "        if self.normalize:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.data = self.scaler.fit_transform(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Calculate dataset length considering input and forecast horizons\n",
    "        return len(self.data) - self.seq_length - self.forecast_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract input (X) and target (Y) sequences\n",
    "        x = self.data[idx: idx + self.seq_length]\n",
    "        y = self.data[idx + self.seq_length: idx + self.seq_length + self.forecast_length]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6300e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic Dataset Download Function\n",
    "def download_datasets(dataset_names, save_path=\"datasets\"):\n",
    "    \"\"\"\n",
    "    Automatically download multiple datasets for MTS forecasting.\n",
    "\n",
    "    Args:\n",
    "        dataset_names (list): List of dataset names to download (e.g., ['ETT', 'METR-LA']).\n",
    "        save_path (str): Directory to save the downloaded datasets.\n",
    "\n",
    "    Returns:\n",
    "        dict: Paths to the downloaded datasets.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    dataset_urls = {\n",
    "        \"ETT\": \"https://raw.githubusercontent.com/zhouhaoyi/ETDataset/main/ETT-small/ETTm2.csv\",\n",
    "        \"METR-LA\": \"https://raw.githubusercontent.com/liyaguang/DCRNN/master/data/sensor_graph/adj_mx.pkl\",\n",
    "        \"PEMS-BAY\": \"https://raw.githubusercontent.com/liyaguang/DCRNN/master/data/sensor_graph/adj_mx_bay.pkl\"\n",
    "    }\n",
    "\n",
    "    downloaded_paths = {}\n",
    "    for dataset_name in dataset_names:\n",
    "        if dataset_name not in dataset_urls:\n",
    "            print(f\"Dataset {dataset_name} is not available for automatic download.\")\n",
    "            continue\n",
    "\n",
    "        url = dataset_urls[dataset_name]\n",
    "        file_name = os.path.join(save_path, os.path.basename(url))\n",
    "\n",
    "        # Download the dataset if it doesn't already exist\n",
    "        if not os.path.exists(file_name):\n",
    "            print(f\"Downloading {dataset_name} dataset from {url}...\")\n",
    "            try:\n",
    "                response = requests.get(url, stream=True)\n",
    "                response.raise_for_status()  # Raise an HTTPError for bad responses\n",
    "                with open(file_name, \"wb\") as f:\n",
    "                    for chunk in response.iter_content(chunk_size=1024):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "                print(f\"{dataset_name} dataset downloaded successfully.\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Failed to download {dataset_name} dataset. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        downloaded_paths[dataset_name] = file_name\n",
    "\n",
    "    return downloaded_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17c455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset Function with Format-Specific Handling\n",
    "def load_data(dataset_name, file_path, delimiter=','):\n",
    "    \"\"\"\n",
    "    Load and preprocess datasets, adapting to specific formats (e.g., .csv, .pkl).\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset being loaded.\n",
    "        file_path (str): Path to the dataset file.\n",
    "        delimiter (str): Delimiter used in the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray or dictionary: Loaded dataset as a NumPy array (for numerical datasets) \n",
    "                                      or dictionary (for .pkl datasets).\n",
    "    \"\"\"\n",
    "    print(f\"Loading {dataset_name} dataset from {file_path}...\")\n",
    "\n",
    "    try:\n",
    "        # Handle binary `.pkl` files (e.g., METR-LA, PEMS-BAY)\n",
    "        if dataset_name in [\"METR-LA\", \"PEMS-BAY\"]:\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                data = pickle.load(f, encoding=\"latin1\")  # Use 'latin1' to properly decode non-ASCII characters\n",
    "            print(f\"{dataset_name} dataset loaded successfully as a dictionary.\")\n",
    "            return data\n",
    "\n",
    "        # Handle CSV files (e.g., ETT)\n",
    "        elif dataset_name == \"ETT\":\n",
    "            data = pd.read_csv(file_path, sep=delimiter, on_bad_lines=\"skip\")\n",
    "            if isinstance(data.iloc[0, 0], str):\n",
    "                print(\"Detected timestamp column. Excluding it from the dataset.\")\n",
    "                data = data.iloc[:, 1:]  # Exclude timestamp column\n",
    "            if data.empty:\n",
    "                raise ValueError(f\"The dataset at {file_path} is empty or not properly formatted.\")\n",
    "            return data.values\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Loading logic for {dataset_name} is not yet implemented.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: An issue occurred while loading the {dataset_name} dataset.\")\n",
    "        print(f\"Details: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae80926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage: Download and Load Multiple Datasets\n",
    "try:\n",
    "    dataset_names = [\"ETT\", \"METR-LA\", \"PEMS-BAY\"]\n",
    "\n",
    "    downloaded_paths = download_datasets(dataset_names)\n",
    "\n",
    "    loaded_data = {}\n",
    "    for dataset_name, file_path in downloaded_paths.items():\n",
    "        raw_data = load_data(dataset_name, file_path)\n",
    "\n",
    "        if dataset_name == \"ETT\":\n",
    "            seq_length = 12  # Length of historical input sequence\n",
    "            forecast_length = 12  # Length of prediction horizon\n",
    "            dataset = MTSDataset(raw_data, seq_length=seq_length, forecast_length=forecast_length)\n",
    "            loaded_data[dataset_name] = dataset\n",
    "            print(f\"ETT Dataset initialized with {len(dataset)} samples.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d04258",
   "metadata": {},
   "source": [
    "BasicTS+ Benchmark: Unified Training Pipeline (Incorporating dataloaders, runners, normalization, training tricks, and evaluation standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415942a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTSPlusTrainer:\n",
    "    \"\"\"\n",
    "    Unified Training Pipeline for BasicTS+ Benchmark.\n",
    "    \n",
    "    Incorporates standardized data loading, normalization, curriculum learning, gradient clipping,\n",
    "    and evaluation standardization for fair and reproducible benchmarking.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, dataloader, optimizer, criterion, scheduler=None, \n",
    "                 clip_grad=1.0, use_curriculum=True, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        \"\"\"\n",
    "        Initialize the unified training pipeline.\n",
    "        \n",
    "        Args:\n",
    "            model (torch.nn.Module): The time series forecasting model.\n",
    "            dataloader (torch.utils.data.DataLoader): Dataloader for training data.\n",
    "            optimizer (torch.optim.Optimizer): Optimizer for model training.\n",
    "            criterion (torch.nn.Module): Loss function.\n",
    "            scheduler (torch.optim.lr_scheduler, optional): Learning rate scheduler. Defaults to None.\n",
    "            clip_grad (float): Gradient clipping value. Defaults to 1.0.\n",
    "            use_curriculum (bool): Whether to use curriculum learning. Defaults to True.\n",
    "            device (str): Computation device. Defaults to \"cuda\" if available.\n",
    "        \"\"\"\n",
    "        self.model = model.to(device)\n",
    "        self.dataloader = dataloader\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.scheduler = scheduler\n",
    "        self.clip_grad = clip_grad\n",
    "        self.use_curriculum = use_curriculum\n",
    "        self.device = device\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        \"\"\"\n",
    "        Perform a single training step.\n",
    "        \n",
    "        Args:\n",
    "            batch (tuple): Contains (input sequence, target sequence).\n",
    "        \n",
    "        Returns:\n",
    "            float: Training loss for the batch.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        x, y = batch\n",
    "        x, y = x.to(self.device), y.to(self.device)\n",
    "        \n",
    "        output = self.model(x)\n",
    "        loss = self.criterion(output, y)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Apply gradient clipping\n",
    "        if self.clip_grad:\n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(), self.clip_grad)\n",
    "\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "    def train(self, epochs=50):\n",
    "        \"\"\"\n",
    "        Train the model for a given number of epochs using curriculum learning.\n",
    "        \n",
    "        Args:\n",
    "            epochs (int): Number of training epochs. Defaults to 50.\n",
    "        \"\"\"\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            total_loss = 0.0\n",
    "            for batch_idx, batch in enumerate(self.dataloader):\n",
    "                # Implement curriculum learning (optional)\n",
    "                if self.use_curriculum and epoch < epochs // 2:\n",
    "                    batch = self.modify_batch_difficulty(batch, factor=epoch / epochs)\n",
    "                \n",
    "                batch_loss = self.train_step(batch)\n",
    "                total_loss += batch_loss\n",
    "\n",
    "            avg_loss = total_loss / len(self.dataloader)\n",
    "            \n",
    "            if self.scheduler:\n",
    "                self.scheduler.step()\n",
    "\n",
    "            print(f\"Epoch {epoch}/{epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    def modify_batch_difficulty(self, batch, factor):\n",
    "        \"\"\"\n",
    "        Adjust batch difficulty in curriculum learning.\n",
    "        \n",
    "        Args:\n",
    "            batch (tuple): Contains (input sequence, target sequence).\n",
    "            factor (float): Difficulty scaling factor.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Modified batch.\n",
    "        \"\"\"\n",
    "        x, y = batch\n",
    "        scaled_x = x * factor  # Scale input data based on progression\n",
    "        return scaled_x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af8d30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage:\n",
    "def initialize_training_pipeline(model, train_data, lr=0.001):\n",
    "    \"\"\"\n",
    "    Initialize BasicTS+ training pipeline with model and training dataset.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Time series forecasting model.\n",
    "        train_data (torch.utils.data.Dataset): Training dataset.\n",
    "        lr (float): Learning rate. Defaults to 0.001.\n",
    "    \n",
    "    Returns:\n",
    "        BasicTSPlusTrainer: Initialized training pipeline.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.7)\n",
    "    \n",
    "    trainer = BasicTSPlusTrainer(model, dataloader, optimizer, criterion, scheduler)\n",
    "    return trainer\n",
    "\n",
    "print(\"BasicTS+ Benchmark: Unified Training Pipeline initialized successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a55fcd",
   "metadata": {},
   "source": [
    "Temporal and Spatial Heterogeneity Analysis (Including methods to categorize datasets and calculate r1 and r2 metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e21fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeterogeneityAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyze temporal and spatial heterogeneity in MTS datasets.\n",
    "\n",
    "    Args:\n",
    "        data (numpy.ndarray): Time series dataset (shape: [samples, variables]).\n",
    "        seq_length (int): Length of historical sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, seq_length, upper_threshold=0.7, lower_threshold=0.1):\n",
    "        if data is None or data.shape[0] < seq_length:\n",
    "            raise ValueError(\"Dataset must contain sufficient time steps.\")\n",
    "\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "        self.upper_threshold = upper_threshold\n",
    "        self.lower_threshold = lower_threshold\n",
    "\n",
    "    def compute_similarity_matrices(self):\n",
    "        \"\"\"\n",
    "        Compute historical similarity (AP) and future similarity (AF) matrices.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Historical similarity matrix (AP), Future similarity matrix (AF).\n",
    "        \"\"\"\n",
    "        num_samples, num_vars = self.data.shape\n",
    "        sample_count = num_samples - self.seq_length\n",
    "\n",
    "        X_hist = np.array([self.data[i:i+self.seq_length] for i in range(sample_count)])\n",
    "        X_future = np.array([self.data[i+self.seq_length] for i in range(sample_count)])\n",
    "\n",
    "        AP = np.array([[np.dot(X_hist[i].flatten(), X_hist[j].flatten()) /\n",
    "                        (np.linalg.norm(X_hist[i].flatten()) * np.linalg.norm(X_hist[j].flatten()))\n",
    "                        for j in range(sample_count)] for i in range(sample_count)])\n",
    "\n",
    "        AF = np.array([[np.dot(X_future[i], X_future[j]) /\n",
    "                        (np.linalg.norm(X_future[i]) * np.linalg.norm(X_future[j]))\n",
    "                        for j in range(sample_count)] for i in range(sample_count)])\n",
    "\n",
    "        return AP, AF\n",
    "\n",
    "    def analyze_similarity_distribution(self, matrix, title):\n",
    "        \"\"\"\n",
    "        Analyze the distribution of similarity scores in AP/AF matrices.\n",
    "\n",
    "        Args:\n",
    "            matrix (numpy.ndarray): Similarity matrix.\n",
    "            title (str): Title for visualization.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.histplot(matrix.flatten(), bins=50, kde=True)\n",
    "        plt.title(f\"Distribution of Similarity Scores - {title}\")\n",
    "        plt.xlabel(\"Similarity Score\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"{title} - Min: {np.min(matrix)}, Max: {np.max(matrix)}, Mean: {np.mean(matrix)}, Std: {np.std(matrix)}\")\n",
    "\n",
    "    def compute_dynamic_thresholds(self, matrix):\n",
    "        \"\"\"\n",
    "        Compute dataset-adaptive thresholds using percentiles.\n",
    "\n",
    "        Args:\n",
    "            matrix (numpy.ndarray): Similarity matrix.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Dynamic upper and lower thresholds.\n",
    "        \"\"\"\n",
    "        upper = np.percentile(matrix, 90)  # Top 10% most similar samples\n",
    "        lower = np.percentile(matrix, 10)  # Bottom 10% least similar samples\n",
    "        return upper, lower\n",
    "\n",
    "    def compute_r1_r2_metrics(self):\n",
    "        \"\"\"\n",
    "        Compute r1 and r2 metrics for spatial indistinguishability analysis.\n",
    "\n",
    "        Returns:\n",
    "            tuple: r1 and r2 values.\n",
    "        \"\"\"\n",
    "        AP, AF = self.compute_similarity_matrices()\n",
    "\n",
    "        # Analyze the distribution of similarity matrices\n",
    "        self.analyze_similarity_distribution(AP, \"Historical (AP)\")\n",
    "        self.analyze_similarity_distribution(AF, \"Future (AF)\")\n",
    "\n",
    "        # Compute dynamic thresholds\n",
    "        dyn_upper_threshold, dyn_lower_threshold = self.compute_dynamic_thresholds(AP)\n",
    "        print(f\"Dynamic Upper Threshold: {dyn_upper_threshold:.4f}, Dynamic Lower Threshold: {dyn_lower_threshold:.4f}\")\n",
    "\n",
    "        total_samples = AP.shape[0] * AP.shape[1]\n",
    "        similar_hist_count = np.sum(AP > dyn_upper_threshold)\n",
    "        indistinguishable_count = np.sum((AP > dyn_upper_threshold) & (AF < dyn_lower_threshold))\n",
    "\n",
    "        # Prevent division by zero errors\n",
    "        r1 = indistinguishable_count / total_samples if total_samples != 0 else 0\n",
    "        r2 = indistinguishable_count / similar_hist_count if similar_hist_count != 0 else 0\n",
    "\n",
    "        return r1, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d07cfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Functions\n",
    "def visualize_temporal_patterns(dataset, method=\"tsne\"):\n",
    "    \"\"\"\n",
    "    Visualize temporal patterns in datasets using t-SNE or KDE.\n",
    "    \n",
    "    Args:\n",
    "        dataset (numpy.ndarray): Time series dataset.\n",
    "        method (str): Visualization method (\"tsne\" or \"kde\").\n",
    "    \"\"\"\n",
    "    if method == \"tsne\":\n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "        transformed_data = tsne.fit_transform(dataset)\n",
    "        plt.scatter(transformed_data[:, 0], transformed_data[:, 1], alpha=0.6)\n",
    "        plt.title(\"t-SNE Visualization of Temporal Patterns\")\n",
    "        plt.xlabel(\"Dimension 1\")\n",
    "        plt.ylabel(\"Dimension 2\")\n",
    "        plt.show()\n",
    "\n",
    "    elif method == \"kde\":\n",
    "        density = gaussian_kde(dataset.flatten())\n",
    "        x_vals = np.linspace(np.min(dataset), np.max(dataset), 100)\n",
    "        plt.plot(x_vals, density(x_vals))\n",
    "        plt.title(\"Kernel Density Estimation of Temporal Patterns\")\n",
    "        plt.xlabel(\"Time Series Values\")\n",
    "        plt.ylabel(\"Density\")\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported visualization method. Choose 'tsne' or 'kde'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f113dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Functions\n",
    "def visualize_temporal_patterns(dataset, method=\"tsne\"):\n",
    "    \"\"\"\n",
    "    Visualize temporal patterns in datasets using t-SNE or KDE.\n",
    "    \n",
    "    Args:\n",
    "        dataset (numpy.ndarray): Time series dataset.\n",
    "        method (str): Visualization method (\"tsne\" or \"kde\").\n",
    "    \"\"\"\n",
    "    if method == \"tsne\":\n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "        transformed_data = tsne.fit_transform(dataset)\n",
    "        plt.scatter(transformed_data[:, 0], transformed_data[:, 1], alpha=0.6)\n",
    "        plt.title(\"t-SNE Visualization of Temporal Patterns\")\n",
    "        plt.xlabel(\"Dimension 1\")\n",
    "        plt.ylabel(\"Dimension 2\")\n",
    "        plt.show()\n",
    "\n",
    "    elif method == \"kde\":\n",
    "        density = gaussian_kde(dataset.flatten())\n",
    "        x_vals = np.linspace(np.min(dataset), np.max(dataset), 100)\n",
    "        plt.plot(x_vals, density(x_vals))\n",
    "        plt.title(\"Kernel Density Estimation of Temporal Patterns\")\n",
    "        plt.xlabel(\"Time Series Values\")\n",
    "        plt.ylabel(\"Density\")\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported visualization method. Choose 'tsne' or 'kde'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d95992",
   "metadata": {},
   "source": [
    "# Transformer Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad43185",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based Model for Multivariate Time Series Forecasting.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Number of input features.\n",
    "        model_dim (int): Dimension of the Transformer embeddings.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        num_layers (int): Number of Transformer encoder layers.\n",
    "        hidden_dim (int): Dimension of hidden layers in feed-forward network.\n",
    "        dropout (float): Dropout rate.\n",
    "        batch_first (bool): Whether the batch dimension comes first in input tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, model_dim=128, num_heads=8, num_layers=4, hidden_dim=256, dropout=0.1, batch_first=True):\n",
    "        super(TimeSeriesTransformer, self).__init__()\n",
    "\n",
    "        # Input embedding layer\n",
    "        self.input_projection = nn.Linear(input_dim, model_dim)\n",
    "\n",
    "        # Positional Encoding to preserve temporal dependencies\n",
    "        self.positional_encoding = PositionalEncoding(model_dim)\n",
    "\n",
    "        # Transformer Encoder Layers (Explicit batch_first=True)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, \n",
    "                                                    dim_feedforward=hidden_dim, dropout=dropout, batch_first=batch_first)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "\n",
    "        # Output projection\n",
    "        self.output_layer = nn.Linear(model_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the Transformer model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input time series tensor (shape: [batch_size, seq_length, input_dim]).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Forecasted values (shape: [batch_size, pred_length, input_dim]).\n",
    "        \"\"\"\n",
    "        x = self.input_projection(x)  # Project input into model dimension\n",
    "        x = self.positional_encoding(x)  # Add positional encoding\n",
    "        x = self.transformer_encoder(x)  # Pass through Transformer encoder\n",
    "        x = self.output_layer(x)  # Map back to original feature dimension\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee025b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Positional Encoding for preserving sequence order in Transformer.\n",
    "    \n",
    "    Args:\n",
    "        model_dim (int): Dimension of the Transformer model.\n",
    "        max_seq_length (int): Maximum expected sequence length.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_dim, max_seq_length=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_seq_length, model_dim)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, model_dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / model_dim))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Adds positional encoding to input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, seq_length, model_dim].\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Positional encoding applied tensor.\n",
    "        \"\"\"\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839e3b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialization Example\n",
    "input_dim = 10  # Example: Number of features in time series\n",
    "model = TimeSeriesTransformer(input_dim=input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c03f7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm model setup\n",
    "print(f\"TimeSeriesTransformer initialized with {sum(p.numel() for p in model.parameters())} parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4efafd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for CUDA support\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Model moved to {device}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "panda38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
